{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jocelynshek/Desktop/RRP/rrpenv/lib/python3.11/site-packages/ipykernel/kernelbase.py:476: RuntimeWarning: coroutine 'InteractiveShell.run_cell_async' was never awaited\n",
      "  self.log.error(\"KeyboardInterrupt caught in kernel.\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script downloads publicly available documents from the Colombian JEP \"Macrocasos\"\n",
    "(Case 01â€“11) section and extracts readable text.\n",
    "\n",
    "How it works:\n",
    "    1. Visits each Case page (caso01.html through caso11.html)\n",
    "    2. Saves HTML pages and any linked PDFs locally\n",
    "    3. Extracts text from each file into plain .txt files\n",
    "    4. Creates a summary CSV index of everything collected\n",
    "    5. Merges all text files into one big searchable document\n",
    "\n",
    "Outputs:\n",
    "    jep_all_macrocasos.csv         â†’ index of all URLs and files\n",
    "    data/html/                     â†’ saved webpage copies\n",
    "    data/pdfs/                     â†’ downloaded PDFs\n",
    "    data/texts/                    â†’ extracted text files\n",
    "    data/all_macrocasos_text.txt   â†’ combined text corpus\n",
    "'''\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Import necessary libraries\n",
    "# -----------------------------\n",
    "import os # working with file paths\n",
    "import re # cleaning/formatting file names\n",
    "import time # pausing between downloads\n",
    "import csv\n",
    "import requests # downloading pages and PDFs from the web\n",
    "from bs4 import BeautifulSoup # extracting text and links from HTML\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Configuration\n",
    "# -----------------------------\n",
    "BASE = \"https://www.jep.gov.co\"\n",
    "START = \"https://www.jep.gov.co/macrocasos/caso01.html\" # first case page\n",
    "DOMAIN = urlparse(BASE).netloc\n",
    "HEADERS = {\"User-Agent\": \"RRP_Crawler/1.0 (contact via Harvard University)\"} #IDs as academic\n",
    "\n",
    "# directories\n",
    "os.makedirs(\"data/html\", exist_ok=True)\n",
    "os.makedirs(\"data/pdfs\", exist_ok=True)\n",
    "os.makedirs(\"data/texts\", exist_ok=True)\n",
    "\n",
    "# These sets/lists keep track of progress\n",
    "visited = set() # URLs already crawled (to avoid duplicates)\n",
    "results = [] #summary info that will later be written to CSV\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Helper functions\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"Downloads a page or PDF and handles common network errors.\n",
    "     Returns the requests.Response object or None on failure.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching:\", url, e)\n",
    "        return None\n",
    "\n",
    "def save_text_file(path, text):\n",
    "    \"\"\"\n",
    "    Saves a string of text into a file (used for both HTML and extracted text).\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def parse_html(url, html):\n",
    "    \"\"\"Turns webpage (HTML) into two things:\n",
    "        1. All visible text (paragraphs, headings, list items)\n",
    "        2. A list of all links found on the page\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    text = \" \".join(p.get_text(\" \", strip=True) for p in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"]))\n",
    "    links = [urljoin(url, a[\"href\"]) for a in soup.find_all(\"a\", href=True)]\n",
    "    return text, links\n",
    "\n",
    "def process_pdf(url):\n",
    "    \"\"\"\n",
    "    Downloads a PDF (if not already saved), extracts its text,\n",
    "    and adds an entry to the results list.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "    pdf_path = os.path.join(\"data/pdfs\", filename)\n",
    "    text_path = os.path.join(\"data/texts\", filename.replace(\".pdf\", \".txt\"))\n",
    "\n",
    "    # Skip downloading if the file already exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(\"Downloading PDF:\", filename)\n",
    "        r = fetch(url)\n",
    "        if r and r.content:\n",
    "            with open(pdf_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            time.sleep(1.0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Try to extract text from the PDF\n",
    "    try:\n",
    "        text = extract_text(pdf_path)\n",
    "        save_text_file(text_path, text)\n",
    "        results.append({\"url\": url, \"type\": \"pdf\", \"file\": pdf_path, \"text_file\": text_path})\n",
    "        print(f\"Extracted PDF: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(\"PDF extract failed:\", filename, e)\n",
    "\n",
    "def crawl(url, depth=0, max_depth=2):\n",
    "    \"\"\"\n",
    "    Recursively explores pages starting from a given URL.\n",
    "    - depth: how far down the link chain we are (0 = start page)\n",
    "    - max_depth: how many link levels to follow (to prevent infinite crawling)\n",
    "    \"\"\"\n",
    "    if url in visited or depth > max_depth:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    print(f\"Crawling ({depth}): {url}\")\n",
    "    resp = fetch(url)\n",
    "    if not resp:\n",
    "        return\n",
    "    # Check file type. If PDF, handle. If not HTML, skip\n",
    "    content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "    if \"pdf\" in content_type or url.lower().endswith(\".pdf\"):\n",
    "        process_pdf(url)\n",
    "        return\n",
    "\n",
    "    if \"html\" not in content_type:\n",
    "        return\n",
    "\n",
    "    # Save HTML text\n",
    "    text, links = parse_html(url, resp.text)\n",
    "    slug = re.sub(r\"\\W+\", \"_\", url.replace(BASE, \"\").strip(\"/\"))[:80]\n",
    "    html_path = os.path.join(\"data/html\", f\"{slug or 'index'}.html\")\n",
    "    text_path = os.path.join(\"data/texts\", f\"{slug or 'index'}.txt\")\n",
    "\n",
    "    save_text_file(html_path, resp.text)\n",
    "    save_text_file(text_path, text)\n",
    "\n",
    "    results.append({\"url\": url, \"type\": \"html\", \"file\": html_path, \"text_file\": text_path})\n",
    "    print(f\"Saved HTML: {url}\")\n",
    "\n",
    "    # Crawl sublinks\n",
    "    for link in links:\n",
    "        parsed = urlparse(link)\n",
    "        if DOMAIN in parsed.netloc and link.startswith(BASE):\n",
    "            crawl(link, depth + 1, max_depth)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Main code: when the script is executed\n",
    "# -------------------------------------------------------\n",
    "\"\"\"\n",
    "    Coordinates the full process:\n",
    "        - Loops through Cases 01â€“11\n",
    "        - Runs the crawler\n",
    "        - Saves a summary CSV\n",
    "        - Merges all text files into one document for future searching\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    #all case URLs 01-11\n",
    "    cases = [f\"https://www.jep.gov.co/macrocasos/caso{str(i).zfill(2)}.html\" for i in range(1, 12)]\n",
    "    for start_url in cases:\n",
    "        print(f\"\\n=== Crawling {start_url} ===\")\n",
    "        crawl(start_url, depth=0, max_depth=2)\n",
    "\n",
    "    # Write summary CSV\n",
    "    with open(\"jep_all_macrocasos.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"url\", \"type\", \"file\", \"text_file\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(\"\\nCrawl complete!\")\n",
    "    print(f\"Visited {len(visited)} URLs; saved {len(results)} files.\")\n",
    "    print(\"Results written to jep_all_macrocasos.csv\")\n",
    "\n",
    "    # Merge all individual text files into one big file\n",
    "    merged_path = \"data/all_macrocasos_text.txt\"\n",
    "    with open(merged_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for root, _, files in os.walk(\"data/texts\"):\n",
    "            for file in sorted(files):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(root, file), encoding=\"utf-8\") as f:\n",
    "                        out.write(f\"\\n\\n===== {file} =====\\n\")\n",
    "                        out.write(f.read())\n",
    "    print(f\"\\nðŸ§¾ Merged all text files into {merged_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Visited 173 URLs; saved 129 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 files containing at least one keyword.\n",
      "Results saved to keyword_hits.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keyword finder: this uses the sexual violence keyword list to scan the text files,\n",
    "and puts into a spreadsheet the ones that mention any of the keywords. The goal is to help\n",
    "others who know Spanish quickly locate documents that may contain relevant subject matter.\n",
    "\n",
    "Process:\n",
    "    - Normalizes accents (so \"violaciÃ³n\" and \"violacion\" both match)\n",
    "    - Uses lowercase comparison for consistency\n",
    "    - Lists which keywords appear in each file\n",
    "    - Saves results to keyword_hits.csv with a short text snippet so you can look where it's found\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import unicodedata\n",
    "\n",
    "# Keyword list, accents removed for normalization\n",
    "KEYWORDS = [\n",
    "    \"violencia sexual\",\n",
    "    \"abuso sexual\",\n",
    "    \"penetracion\",\n",
    "    \"desnuda\",\n",
    "    \"abusaron\",\n",
    "    \"acoso\",\n",
    "    \"relaciones sexuales\",\n",
    "    \"enamoradita\",\n",
    "    \"acceso carnal violento\",\n",
    "    \"prostituta\",\n",
    "    \"prostitucion\",\n",
    "    \"actos sexuales\",\n",
    "    \"violacion sexual\",\n",
    "    \"abuso\",\n",
    "    \"abusada\",\n",
    "    \"abusado\"\n",
    "]\n",
    "\n",
    "INPUT_FOLDER = \"data/texts\"\n",
    "OUTPUT_CSV = \"keyword_hits.csv\"\n",
    "\n",
    "# Normalization function\n",
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Lowercase and remove accent marks from a string\n",
    "    (so 'violaciÃ³n' -> 'violacion').\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Decompose accented characters, then filter out the accents\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))\n",
    "    return text\n",
    "\n",
    "# Scan text files\n",
    "rows = []\n",
    "\n",
    "for file in os.listdir(INPUT_FOLDER):\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "    path = os.path.join(INPUT_FOLDER, file)\n",
    "\n",
    "    # Read and normalize the file text\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = normalize(f.read())\n",
    "\n",
    "    # Find which keywords appear\n",
    "    hits = [kw for kw in KEYWORDS if kw in text]\n",
    "    if hits:\n",
    "        snippet = text[:500].replace(\"\\n\", \" \")\n",
    "        rows.append({\n",
    "            \"file\": file,\n",
    "            \"keywords_found\": \", \".join(hits),\n",
    "            \"snippet\": snippet\n",
    "        })\n",
    "\n",
    "# Save matches to CSV\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"file\", \"keywords_found\", \"snippet\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Found {len(rows)} files containing at least one keyword.\")\n",
    "print(f\"Results saved to {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
